{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Log Anomaly Detection Using LogAI\n",
    "\n",
    "This is an example to show how to use LogAI to conduct log anomaly detection analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "You can use `OpensetDataLoader` to load a sample open log dataset. Here we use HealthApp dataset from\n",
    "[LogHub](https://zenodo.org/record/3227177#.Y1M3LezML0o) as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\logai\\dataloader\\data_loader.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logline</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Action</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onExtend:1514038530000 14 0 4</td>\n",
       "      <td>2017-12-23 22:15:29.615</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onReceive action: android.intent.action.SCREEN_ON</td>\n",
       "      <td>2017-12-23 22:15:29.633</td>\n",
       "      <td>Step_StandReportReceiver</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processHandleBroadcastAction action:android.in...</td>\n",
       "      <td>2017-12-23 22:15:29.635</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flush sensor data</td>\n",
       "      <td>2017-12-23 22:15:29.635</td>\n",
       "      <td>Step_StandStepCounter</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getTodayTotalDetailSteps = 1514038440000##699...</td>\n",
       "      <td>2017-12-23 22:15:29.635</td>\n",
       "      <td>Step_SPUtils</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             logline               timestamp  \\\n",
       "0                      onExtend:1514038530000 14 0 4 2017-12-23 22:15:29.615   \n",
       "1  onReceive action: android.intent.action.SCREEN_ON 2017-12-23 22:15:29.633   \n",
       "2  processHandleBroadcastAction action:android.in... 2017-12-23 22:15:29.635   \n",
       "3                                  flush sensor data 2017-12-23 22:15:29.635   \n",
       "4   getTodayTotalDetailSteps = 1514038440000##699... 2017-12-23 22:15:29.635   \n",
       "\n",
       "                     Action        ID  \n",
       "0                  Step_LSC  30002312  \n",
       "1  Step_StandReportReceiver  30002312  \n",
       "2                  Step_LSC  30002312  \n",
       "3     Step_StandStepCounter  30002312  \n",
       "4              Step_SPUtils  30002312  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from logai.dataloader.openset_data_loader import OpenSetDataLoader, OpenSetDataLoaderConfig\n",
    "\n",
    "#File Configuration\n",
    "filepath = os.path.join(\"..\",\"Data\" ,\"HealthApp\", \"HealthApp.log\") # Point to the target HealthApp.log dataset\n",
    "\n",
    "dataset_name = \"HealthApp\"\n",
    "data_loader = OpenSetDataLoader(\n",
    "    OpenSetDataLoaderConfig(\n",
    "        dataset_name=dataset_name,\n",
    "        filepath=filepath)\n",
    ")\n",
    "\n",
    "logrecord = data_loader.load_data()\n",
    "\n",
    "logrecord.to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "In preprocessing step user can retrieve and replace any regex strings and clean the raw loglines. This\n",
    "can be very useful to improve information extraction of the unstructured part of logs,\n",
    " as well as generate more structured attributes with domain knowledge.\n",
    "\n",
    "Here in the example, we use the below regex to retrieve IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;IP&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  <IP>\n",
       "0   []\n",
       "1   []\n",
       "2   []\n",
       "3   []\n",
       "4   []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logai.preprocess.preprocessor import PreprocessorConfig, Preprocessor\n",
    "from logai.utils import constants\n",
    "\n",
    "loglines = logrecord.body[constants.LOGLINE_NAME]\n",
    "attributes = logrecord.attributes\n",
    "\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    custom_replace_list=[\n",
    "        [r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", \"<IP>\"],   # retrieve all IP addresses and replace with <IP> tag in the original string.\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(preprocessor_config)\n",
    "\n",
    "clean_logs, custom_patterns = preprocessor.clean_log(\n",
    "    loglines\n",
    ")\n",
    "custom_patterns.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "After preprocessing, we call auto-parsing algorithms to automatically parse the cleaned logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "\n",
    "# parsing\n",
    "parsing_algo_params = DrainParams(\n",
    "    sim_th=0.5, depth=5\n",
    ")\n",
    "\n",
    "log_parser_config = LogParserConfig(\n",
    "    parsing_algorithm=\"drain\",\n",
    "    parsing_algo_params=parsing_algo_params\n",
    ")\n",
    "\n",
    "parser = LogParser(log_parser_config)\n",
    "parsed_result = parser.parse(clean_logs)\n",
    "# parsed_result\n",
    "parsed_loglines = parsed_result['parsed_logline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logline</th>\n",
       "      <th>parsed_logline</th>\n",
       "      <th>parameter_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onExtend:1514038530000 14 0 4</td>\n",
       "      <td>* * 0 4</td>\n",
       "      <td>[onExtend:1514038530000, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onReceive action: android.intent.action.SCREEN_ON</td>\n",
       "      <td>onReceive action: *</td>\n",
       "      <td>[android.intent.action.SCREEN_ON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processHandleBroadcastAction action:android.in...</td>\n",
       "      <td>processHandleBroadcastAction *</td>\n",
       "      <td>[action:android.intent.action.SCREEN_ON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flush sensor data</td>\n",
       "      <td>flush sensor data</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getTodayTotalDetailSteps = 1514038440000##699...</td>\n",
       "      <td>getTodayTotalDetailSteps = *</td>\n",
       "      <td>[1514038440000##6993##548365##8661##12266##271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253369</th>\n",
       "      <td>calculateCaloriesWithCache totalCalories=52108</td>\n",
       "      <td>calculateCaloriesWithCache *</td>\n",
       "      <td>[totalCalories=52108]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253370</th>\n",
       "      <td>calculateAltitudeWithCache totalAltitude=60</td>\n",
       "      <td>calculateAltitudeWithCache *</td>\n",
       "      <td>[totalAltitude=60]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253371</th>\n",
       "      <td>processHandleBroadcastAction action:android.in...</td>\n",
       "      <td>processHandleBroadcastAction *</td>\n",
       "      <td>[action:android.intent.action.TIME_TICK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253372</th>\n",
       "      <td>processHandleBroadcastAction action:android.in...</td>\n",
       "      <td>processHandleBroadcastAction *</td>\n",
       "      <td>[action:android.intent.action.TIME_TICK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253373</th>\n",
       "      <td>processHandleBroadcastAction action:android.in...</td>\n",
       "      <td>processHandleBroadcastAction *</td>\n",
       "      <td>[action:android.intent.action.TIME_TICK]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253374 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  logline  \\\n",
       "0                           onExtend:1514038530000 14 0 4   \n",
       "1       onReceive action: android.intent.action.SCREEN_ON   \n",
       "2       processHandleBroadcastAction action:android.in...   \n",
       "3                                       flush sensor data   \n",
       "4        getTodayTotalDetailSteps = 1514038440000##699...   \n",
       "...                                                   ...   \n",
       "253369     calculateCaloriesWithCache totalCalories=52108   \n",
       "253370        calculateAltitudeWithCache totalAltitude=60   \n",
       "253371  processHandleBroadcastAction action:android.in...   \n",
       "253372  processHandleBroadcastAction action:android.in...   \n",
       "253373  processHandleBroadcastAction action:android.in...   \n",
       "\n",
       "                        parsed_logline  \\\n",
       "0                              * * 0 4   \n",
       "1                  onReceive action: *   \n",
       "2       processHandleBroadcastAction *   \n",
       "3                    flush sensor data   \n",
       "4         getTodayTotalDetailSteps = *   \n",
       "...                                ...   \n",
       "253369    calculateCaloriesWithCache *   \n",
       "253370    calculateAltitudeWithCache *   \n",
       "253371  processHandleBroadcastAction *   \n",
       "253372  processHandleBroadcastAction *   \n",
       "253373  processHandleBroadcastAction *   \n",
       "\n",
       "                                           parameter_list  \n",
       "0                            [onExtend:1514038530000, 14]  \n",
       "1                       [android.intent.action.SCREEN_ON]  \n",
       "2                [action:android.intent.action.SCREEN_ON]  \n",
       "3                                                      []  \n",
       "4       [1514038440000##6993##548365##8661##12266##271...  \n",
       "...                                                   ...  \n",
       "253369                              [totalCalories=52108]  \n",
       "253370                                 [totalAltitude=60]  \n",
       "253371           [action:android.intent.action.TIME_TICK]  \n",
       "253372           [action:android.intent.action.TIME_TICK]  \n",
       "253373           [action:android.intent.action.TIME_TICK]  \n",
       "\n",
       "[253374 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time-series Anomaly Detection\n",
    "\n",
    "Here we show an example to conduct time-series anomaly detection with parsed logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "After parsing the logs and get log templates, we can extract timeseries features by coverting\n",
    "these parsed loglines into counter vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_logline</th>\n",
       "      <th>Action</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>event_index</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-23 23:00:00</td>\n",
       "      <td>[1347]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-24 11:15:00</td>\n",
       "      <td>[4660]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-24 12:00:00</td>\n",
       "      <td>[6985, 7064]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-24 12:45:00</td>\n",
       "      <td>[7458, 7459, 7473]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-24 15:30:00</td>\n",
       "      <td>[7999, 8000, 8003, 8007, 8008, 8009, 8010, 801...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parsed_logline    Action        ID           timestamp  \\\n",
       "0        * * 0 0  Step_LSC  30002312 2017-12-23 23:00:00   \n",
       "1        * * 0 0  Step_LSC  30002312 2017-12-24 11:15:00   \n",
       "2        * * 0 0  Step_LSC  30002312 2017-12-24 12:00:00   \n",
       "3        * * 0 0  Step_LSC  30002312 2017-12-24 12:45:00   \n",
       "4        * * 0 0  Step_LSC  30002312 2017-12-24 15:30:00   \n",
       "\n",
       "                                         event_index  counts  \n",
       "0                                             [1347]       1  \n",
       "1                                             [4660]       1  \n",
       "2                                       [6985, 7064]       2  \n",
       "3                                 [7458, 7459, 7473]       3  \n",
       "4  [7999, 8000, 8003, 8007, 8008, 8009, 8010, 801...      38  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    group_by_time=\"15min\",\n",
    "    group_by_category=['parsed_logline', 'Action', 'ID'],\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "parsed_loglines = parsed_result['parsed_logline']\n",
    "counter_vector = feature_extractor.convert_to_counter_vector(\n",
    "    log_pattern=parsed_loglines,\n",
    "    attributes=attributes,\n",
    "    timestamps=timestamps\n",
    ")\n",
    "\n",
    "counter_vector.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "With the generated `counter_vcetor`, you can use `AnomalyDetector` to detect timeseries anomalies.\n",
    "Here we use an algorithm in Merlion library called `DynamicBaseLine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.analysis.anomaly_detector import AnomalyDetector, AnomalyDetectionConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "counter_vector[\"attribute\"] = counter_vector.drop(\n",
    "                [\n",
    "                    constants.LOG_COUNTS,\n",
    "                    constants.LOG_TIMESTAMPS,\n",
    "                    constants.EVENT_INDEX\n",
    "                ],\n",
    "                axis=1\n",
    "            ).apply(\n",
    "                lambda x: \"-\".join(x.astype(str)), axis=1\n",
    "            )\n",
    "\n",
    "attr_list = counter_vector[\"attribute\"].unique()\n",
    "\n",
    "anomaly_detection_config = AnomalyDetectionConfig(\n",
    "    algo_name='dbl'\n",
    ")\n",
    "\n",
    "res = pd.DataFrame()\n",
    "for attr in attr_list:\n",
    "    temp_df = counter_vector[counter_vector[\"attribute\"] == attr]\n",
    "    if temp_df.shape[0] >= constants.MIN_TS_LENGTH:\n",
    "        train, test = train_test_split(\n",
    "            temp_df[[constants.LOG_TIMESTAMPS, constants.LOG_COUNTS]],\n",
    "            shuffle=False,\n",
    "            train_size=0.3\n",
    "        )\n",
    "        anomaly_detector = AnomalyDetector(anomaly_detection_config)\n",
    "        anomaly_detector.fit(train)\n",
    "        anom_score = anomaly_detector.predict(test)\n",
    "        # res.loc[len(res)] = anom_score\n",
    "        res = res._append(anom_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anom_score</th>\n",
       "      <th>trainval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    anom_score  trainval\n",
       "42         0.0     False\n",
       "43         0.0     False\n",
       "44         0.0     False\n",
       "45         0.0     False\n",
       "46         0.0     False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_logline</th>\n",
       "      <th>Action</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>event_index</th>\n",
       "      <th>counts</th>\n",
       "      <th>attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-27 12:45:00</td>\n",
       "      <td>[91837, 91876]</td>\n",
       "      <td>2</td>\n",
       "      <td>* * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-27 17:30:00</td>\n",
       "      <td>[93344]</td>\n",
       "      <td>1</td>\n",
       "      <td>* * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-27 20:00:00</td>\n",
       "      <td>[97356]</td>\n",
       "      <td>1</td>\n",
       "      <td>* * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-27 20:30:00</td>\n",
       "      <td>[100183]</td>\n",
       "      <td>1</td>\n",
       "      <td>* * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>* * 0 0</td>\n",
       "      <td>Step_LSC</td>\n",
       "      <td>30002312</td>\n",
       "      <td>2017-12-28 08:30:00</td>\n",
       "      <td>[103117, 103379, 103426]</td>\n",
       "      <td>3</td>\n",
       "      <td>* * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsed_logline    Action        ID           timestamp  \\\n",
       "42        * * 0 0  Step_LSC  30002312 2017-12-27 12:45:00   \n",
       "43        * * 0 0  Step_LSC  30002312 2017-12-27 17:30:00   \n",
       "44        * * 0 0  Step_LSC  30002312 2017-12-27 20:00:00   \n",
       "45        * * 0 0  Step_LSC  30002312 2017-12-27 20:30:00   \n",
       "46        * * 0 0  Step_LSC  30002312 2017-12-28 08:30:00   \n",
       "\n",
       "                 event_index  counts  \\\n",
       "42            [91837, 91876]       2   \n",
       "43                   [93344]       1   \n",
       "44                   [97356]       1   \n",
       "45                  [100183]       1   \n",
       "46  [103117, 103379, 103426]       3   \n",
       "\n",
       "                                            attribute  \n",
       "42  * * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...  \n",
       "43  * * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...  \n",
       "44  * * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...  \n",
       "45  * * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...  \n",
       "46  * * 0 0-Step_LSC-30002312-* * 0 0-Step_LSC-300...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get anomalous datapoints\n",
    "anomalies = counter_vector.iloc[res[res>0].index]\n",
    "anomalies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Semantic Anomaly Detection\n",
    "\n",
    "We can also use the log template for semantic based anomaly detection. In this approach, we retrieve\n",
    "the semantic features from the logs. This includes two parts: vectorizing the unstructured log templates\n",
    "and encoding the structured log attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Vectorization for unstructured loglines\n",
    "\n",
    "Here we use `word2vec` to vectorize unstructured part of the logs. The output will be a list of\n",
    "numeric vectors that representing the semantic features of these log templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.log_vectorizer import VectorizerConfig, LogVectorizer\n",
    "\n",
    "vectorizer_config = VectorizerConfig(\n",
    "    algo_name = \"word2vec\"\n",
    ")\n",
    "\n",
    "vectorizer = LogVectorizer(\n",
    "    vectorizer_config\n",
    ")\n",
    "\n",
    "# Train vectorizer\n",
    "vectorizer.fit(parsed_loglines)\n",
    "\n",
    "# Transform the loglines into features\n",
    "log_vectors = vectorizer.transform(parsed_loglines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Categorical Encoding for log attributes\n",
    "\n",
    "We also do categorical encoding for log attributes to convert the strings into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.categorical_encoder import CategoricalEncoderConfig, CategoricalEncoder\n",
    "\n",
    "encoder_config = CategoricalEncoderConfig(name=\"label_encoder\")\n",
    "\n",
    "encoder = CategoricalEncoder(encoder_config)\n",
    "\n",
    "attributes_encoded = encoder.fit_transform(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Then we extract and concate the semantic features for both the unstructured and structured part of logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    max_feature_len=100\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "_, feature_vector = feature_extractor.convert_to_feature_vector(log_vectors, attributes_encoded, timestamps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "With the extracted log semantic feature set, we can perform anomaly detection to find the abnormal\n",
    "logs. Here we use `isolation_forest` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m config \u001b[39m=\u001b[39m AnomalyDetectionConfig(\n\u001b[0;32m     13\u001b[0m     algo_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39misolation_forest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     algo_params\u001b[39m=\u001b[39malgo_params\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m anomaly_detector \u001b[39m=\u001b[39m AnomalyDetector(config)\n\u001b[1;32m---> 18\u001b[0m anomaly_detector\u001b[39m.\u001b[39;49mfit(train)\n\u001b[0;32m     19\u001b[0m res \u001b[39m=\u001b[39m anomaly_detector\u001b[39m.\u001b[39mpredict(test)\n\u001b[0;32m     20\u001b[0m \u001b[39m# obtain the anomalous datapoints\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\logai\\analysis\\anomaly_detector.py:54\u001b[0m, in \u001b[0;36mAnomalyDetector.fit\u001b[1;34m(self, log_features)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, log_features: pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    Trains an anomaly detection given the training dataset.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[39m    :param log_features: The training dataset.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49manomaly_detector\u001b[39m.\u001b[39;49mfit(log_features)\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\logai\\algorithms\\anomaly_detection_algo\\isolation_forest.py:75\u001b[0m, in \u001b[0;36mIsolationForestDetector.fit\u001b[1;34m(self, log_features)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, log_features: pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m     69\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    Fits an isolation forest model.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[39m    :param log_features: The input for model training.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m    :return: The scores of the training dataset.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(log_features)\n\u001b[0;32m     76\u001b[0m     train_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mscore_samples(log_features)\n\u001b[0;32m     77\u001b[0m     train_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(train_scores, index\u001b[39m=\u001b[39mlog_features\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:290\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    269\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m    Fit estimator.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtree_dtype)\n\u001b[0;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    292\u001b[0m         \u001b[39m# Pre-sort indices to avoid that each individual tree of the\u001b[39;00m\n\u001b[0;32m    293\u001b[0m         \u001b[39m# ensemble sorts the indices.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m         X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Aman Kanojiya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:797\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    793\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    794\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 797\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[0;32m    798\u001b[0m \u001b[39melif\u001b[39;00m pandas_requires_conversion \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(d \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    799\u001b[0m     \u001b[39m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    800\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\n",
      "\u001b[1;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(feature_vector, train_size=0.7, test_size=0.3)\n",
    "\n",
    "from logai.algorithms.anomaly_detection_algo.isolation_forest import IsolationForestParams\n",
    "from logai.analysis.anomaly_detector import AnomalyDetectionConfig, AnomalyDetector\n",
    "\n",
    "algo_params = IsolationForestParams(\n",
    "    n_estimators=10,\n",
    "    max_features=100\n",
    ")\n",
    "config = AnomalyDetectionConfig(\n",
    "    algo_name='isolation_forest',\n",
    "    algo_params=algo_params\n",
    ")\n",
    "\n",
    "anomaly_detector = AnomalyDetector(config)\n",
    "anomaly_detector.fit(train)\n",
    "res = anomaly_detector.predict(test)\n",
    "# obtain the anomalous datapoints\n",
    "anomalies = res[res==1]\n",
    "print(res)\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the corresponding loglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42     getTodayTotalDetailSteps = 1514038440000##701...\n",
       "43    setTodayTotalDetailSteps=1514038440000##7013##...\n",
       "44      calculateCaloriesWithCache totalCalories=126904\n",
       "45         calculateAltitudeWithCache totalAltitude=240\n",
       "46                        REPORT : 7013 5007 150218 240\n",
       "Name: logline, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = loglines.iloc[anomalies.index].head(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the corresponding attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Step_SPUtils</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Step_SPUtils</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Step_ExtSDM</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Step_ExtSDM</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Step_StandReportReceiver</td>\n",
       "      <td>30002312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Action        ID\n",
       "42              Step_SPUtils  30002312\n",
       "43              Step_SPUtils  30002312\n",
       "44               Step_ExtSDM  30002312\n",
       "45               Step_ExtSDM  30002312\n",
       "46  Step_StandReportReceiver  30002312"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.iloc[anomalies.index].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
